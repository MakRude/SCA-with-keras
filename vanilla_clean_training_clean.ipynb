{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d5970bd6dbb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m## CNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m### Model modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAveragePooling1D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAveragePooling1D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/keras/utils/conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/keras/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_epsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfloatx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_floatx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcast_to_floatx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/keras/backend/load_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# Try and load external backend.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfdev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;31m# We still need all the names that are toplevel on tensorflow_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_core\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m# These should not be visible in the main tf module.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;34m\"\"\"Import the target module and insert it into the parent's namespace.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_importlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_column_lib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_layer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minput_layer_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnode_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/input_layer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributed_training_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnode_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/distribute/distributed_training_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m   \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[0mrequests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/requests/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m# Attempt to enable urllib3's SNI support, if possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0murllib3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyopenssl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0mpyopenssl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minject_into_urllib3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/urllib3/contrib/pyopenssl.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcryptography\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mx509\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcryptography\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhazmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenssl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopenssl_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/OpenSSL/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \"\"\"\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mOpenSSL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcrypto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSSL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m from OpenSSL.version import (\n\u001b[1;32m     10\u001b[0m     \u001b[0m__author__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__copyright__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__email__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__license__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__summary__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__title__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/OpenSSL/crypto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     PY3 as _PY3)\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcryptography\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mx509\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcryptography\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhazmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimitives\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masymmetric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdsa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcryptography\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/cryptography/x509/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcryptography\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx509\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcertificate_transparency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from cryptography.x509.base import (\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mCertificate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCertificateBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCertificateRevocationList\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mCertificateRevocationListBuilder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/cryptography/x509/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcryptography\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcryptography\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhazmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimitives\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masymmetric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdsa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcryptography\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx509\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextensions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExtension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExtensionType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcryptography\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx509\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mName\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/cryptography/x509/extensions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcryptography\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcryptography\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhazmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimitives\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstant_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcryptography\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhazmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimitives\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masymmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEllipticCurvePublicKey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcryptography\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhazmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimitives\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masymmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsa\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRSAPublicKey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/cryptography/hazmat/primitives/constant_time.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcryptography\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcryptography\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhazmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbindings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constant_time\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This code simply loads traces and displays them in a spectrogram and in a plot\n",
    "# It does not contain any training functions or parameters\n",
    "# It was created to increase the readability of the main code and to enable the display of the loaded traces (Duh :P)\n",
    "\n",
    "# This code contains code adapted from Nils Wisiol, sntrup4591761 and ANSSI-FR/ASCAD\n",
    "# All snippets are marked accordingly as such\n",
    "\n",
    "# General modules\n",
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "from pathlib import Path # creating directories\n",
    "\n",
    "# Load data modules\n",
    "from numpy import empty, zeros, uint16\n",
    "from numpy.random import RandomState\n",
    "import numpy as np\n",
    "## load cw data\n",
    "import chipwhisperer as cw\n",
    "## load ASCAD data\n",
    "import h5py\n",
    "\n",
    "\n",
    "# Plot traces\n",
    "from scipy import signal, fftpack\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training modules\n",
    "## CNN\n",
    "### Model modules\n",
    "from keras.layers import Input, Conv1D, AveragePooling1D, Flatten, Dense, AveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "### training method modules\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "\n",
    "## CNN2\n",
    "### Model modules\n",
    "from keras.layers import BatchNormalization, GaussianNoise, MaxPooling1D, Dropout\n",
    "\n",
    "## MLP\n",
    "### Model modules\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential\n",
    "\n",
    "# plot result\n",
    "from seaborn import catplot\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Streamlined NTRU Prime: sntrup4591761\n",
    "# # # Unmodified and unused\n",
    "\n",
    "# P, Q, W = 761, 4591, 286\n",
    "\n",
    "# def key_gen(seed):\n",
    "# \t\"\"\" Returns a Streamlined NTRU Prime key. (But does not check invertability) \"\"\"\n",
    "# \t# TODO see if this is a good approximation of the key distribution\n",
    "# \t# this is supposed to give a random, small element of R,\n",
    "# \t# R = Z[x]/(x**p−x−1), that is invertible in R/3.\n",
    "# \t# TODO this does not check for invertibility\n",
    "\n",
    "# \tr = RandomState(seed)\n",
    "# \tc_values = 2 * r.randint(2, size=W) - 1  # W \"small\" non-zero coefficients, i.e. -1, 1\n",
    "# \tc_pos = r.choice(P, W, replace=False)  # choose W positions out of the P possible ones\n",
    "# \tcs = zeros(P, dtype=uint16)\n",
    "# \tfor i in range(W):  # fill the W non-zero values\n",
    "# \t\tcs[c_pos[i]] = c_values[i]\n",
    "# \treturn cs\n",
    "\n",
    "# def text_gen(seed):\n",
    "# \t\"\"\" Returns a uniformly random from the space of all Streamlined NTRU Prime Ciphertexsts. \"\"\"\n",
    "# \t# uniformly random element of R with coefficients being only\n",
    "# \t# multiple of 3\n",
    "# \tr = RandomState(seed)\n",
    "# \treturn (r.randint(0, Q, size=P) * 3 // 3) % Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASCAD: Adapted by Mahmoud Gharra to fit the NTRU Prime input\n",
    "\n",
    "# returns traces and labels, as well as some important global meta_data\n",
    "def load_database_cw(my_database):\n",
    "    # load traces\n",
    "    print(\"++ Loading projects\")\n",
    "    project = cw.open_project(my_database)\n",
    "    global KEY_LENGTH, TEXT_LENGTH, SAMPLE_HIGH, TRAINING_SLICE, TEST_NUM, TEST_SLICE\n",
    "\n",
    "    # Organize trace data for MLP\n",
    "    print(\"++ Organizing traces\")\n",
    "    KEY_LENGTH = TEXT_LENGTH = project.keys[0].size\n",
    "\n",
    "    sample_low = 0\n",
    "    SAMPLE_HIGH = project.trace_manager().num_points() # length of singular trace\n",
    "    sample_slice = slice(sample_low, SAMPLE_HIGH)\n",
    "    sample_num = len(project.traces) # number of traces\n",
    "#     print(\"sample num: \", sample_num)\n",
    "    training_num = sample_num - tst_len\n",
    "    TRAINING_SLICE = slice(0, training_num)\n",
    "\n",
    "    TEST_NUM = sample_num - training_num\n",
    "    TEST_SLICE = slice(training_num, TEST_NUM + training_num)\n",
    "    assert TEST_NUM + training_num <= sample_num\n",
    "    assert training_num > 3*TEST_NUM\n",
    "\n",
    "    # organize traces in X matrix\n",
    "    X = empty(shape=(sample_num, SAMPLE_HIGH - sample_low))\n",
    "    for i in range(sample_num):\n",
    "        X[i] = project.waves[i][sample_slice]\n",
    "\n",
    "    # organize the operands in Y matrix\n",
    "    y = empty(shape=(sample_num, KEY_LENGTH))\n",
    "    for i in range(sample_num):\n",
    "        # reproduce values\n",
    "        y[i] = project.keys[i]\n",
    "        # These four lines caused a ton of issues in the past.\n",
    "        # I left them in to remind myself and who ever is reading this not to doubt themselves :p\n",
    "        # You're doing great simply by trying to be a better programmer and practicing your craft.\n",
    "        \n",
    "        # text_num = text_gen(seed=i)[:TEXT_LENGTH]\n",
    "        # key_num = key_gen(seed=i)[:KEY_LENGTH]\n",
    "        # y[i][:] = key_num[:]\n",
    "        # y[i][y[i] > 2**15] -= 2**16\n",
    "        \n",
    "        # In case you're wondering what the issue was in boring detail:\n",
    "        # The original script used the training seed to generate the data on the chip whisperer\n",
    "        # this meant, that you don't need to read out the labels (polynomial coefficients) but you need the seed to train the data\n",
    "        # This made the script unfit to handle new date sets.\n",
    "        \n",
    "    \n",
    "    # this is in case of little endian\n",
    "    y = np.array((y.T[1::2].T*(2**8)) + y.T[::2].T, dtype=np.int16)\n",
    "    KEY_LENGTH = TEXT_LENGTH = int(KEY_LENGTH/2)\n",
    "    \n",
    "    # transform generated key numbers to labels\n",
    "    unique = np.unique(y)\n",
    "    class_dic = dict([[unique[i], i] for i in range(len(unique))])\n",
    "    y_labelled = np.vectorize(class_dic.get)(y)\n",
    "    return X, y_labelled\n",
    "\n",
    "\n",
    "def read_parameters_from_file(param_filename):\n",
    "    #read parameters for the train_model and load_traces functions\n",
    "    #TODO: sanity checks on parameters\n",
    "    param_file = open(param_filename,\"r\")\n",
    "\n",
    "    #TODO: replace eval() by ast.linear_eval()\n",
    "    my_parameters= eval(param_file.read())\n",
    "\n",
    "    my_database = my_parameters[\"database\"]\n",
    "    my_database_title = my_parameters[\"database_title\"]\n",
    "    return my_database_title, my_database\n",
    "\n",
    "def extract_data(X, y, key_idx, best_model):\n",
    "    # Load profiling traces\n",
    "    X_profiling = X[TRAINING_SLICE]\n",
    "    # Load profiling labels\n",
    "    Y_profiling = y[TRAINING_SLICE, key_idx]\n",
    "    # Load testing traces\n",
    "    X_testing = X[TEST_SLICE]\n",
    "    ## make sure input data has correct shape (this part was adapted from ASCAD code as we need to do the exact same thing here)\n",
    "    input_layer_shape = best_model.get_layer(index=0).input_shape\n",
    "    # Adapt the data shape according our model input\n",
    "    if len(input_layer_shape) == 2:\n",
    "        # This is a MLP\n",
    "        X_testing = X[TEST_SLICE, :]\n",
    "    elif len(input_layer_shape) == 3:\n",
    "        # This is a CNN: reshape the data\n",
    "        X_testing = X[TEST_SLICE, :]\n",
    "        X_testing = X_testing.reshape((X_testing.shape[0], X_testing.shape[1], 1))\n",
    "    else:\n",
    "        print(\"Error: model input shape length %d is not expected ...\" % len(input_layer_shape))\n",
    "        sys.exit(-1)\n",
    "    # Load testing labels\n",
    "    Y_testing = y[TEST_SLICE, key_idx]\n",
    "    return (X_profiling, Y_profiling), (X_testing, Y_testing)\n",
    "\n",
    "# https://www.oreilly.com/library/view/elegant-scipy/9781491922927/ch04.html\n",
    "def four_tr(traces, tmp_i, f_s):\n",
    "    _Four = fftpack.fft(traces[tmp_i])\n",
    "    _freqs = fftpack.fftfreq(len(traces[tmp_i])) * f_s\n",
    "    \n",
    "    return _Four, _freqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unaltered code snippet taken from ASCAD_train_model\n",
    "\n",
    "#### ASCAD helper to load profiling and attack data (traces and labels)\n",
    "# Loads the profiling and attack datasets from the ASCAD\n",
    "# database\n",
    "def load_ascad(ascad_database_file, load_metadata=False):\n",
    "\tcheck_file_exists(ascad_database_file)\n",
    "\t# Open the ASCAD database HDF5 for reading\n",
    "\ttry:\n",
    "\t\tin_file  = h5py.File(ascad_database_file, \"r\")\n",
    "\texcept:\n",
    "\t\tprint(\"Error: can't open HDF5 file '%s' for reading (it might be malformed) ...\" % ascad_database_file)\n",
    "\t\tsys.exit(-1)\n",
    "\t# Load profiling traces\n",
    "\tX_profiling = np.array(in_file['Profiling_traces/traces'], dtype=np.int8)\n",
    "\t# Load profiling labels\n",
    "\tY_profiling = np.array(in_file['Profiling_traces/labels'])\n",
    "\t# Load attacking traces\n",
    "\tX_attack = np.array(in_file['Attack_traces/traces'], dtype=np.int8)\n",
    "\t# Load attacking labels\n",
    "\tY_attack = np.array(in_file['Attack_traces/labels'])\n",
    "\tif load_metadata == False:\n",
    "\t\treturn (X_profiling, Y_profiling), (X_attack, Y_attack)\n",
    "\telse:\n",
    "\t\treturn (X_profiling, Y_profiling), (X_attack, Y_attack), (in_file['Profiling_traces/metadata'], in_file['Attack_traces/metadata'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_traces(unlab_traces):\n",
    "    \n",
    "    t = np.arange(unlab_traces.shape[1])\n",
    "    myTraces = np.array(unlab_traces)\n",
    "\n",
    "    rand_ind = np.zeros(tot)\n",
    "    for i in range(tot):\n",
    "        rand_ind[i] = np.random.randint(myTraces.shape[0])\n",
    "\n",
    "    # plot traces\n",
    "    print(\"++ plot traces\")\n",
    "    fig1, axs1 = plt.subplots(tot)\n",
    "    fig1.suptitle(DB_title + \": \" + str(rand_ind))\n",
    "    for i in range(tot):\n",
    "        axs1[i].plot(t, myTraces[int(rand_ind[i])])\n",
    "\n",
    "    # TODO: This frequency graphs were built by learning how the library was used in the following link:\n",
    "    # https://www.oreilly.com/library/view/elegant-scipy/9781491922927/ch04.html    \n",
    "    # frequency plot\n",
    "    print(\"++ plot frequencies\")\n",
    "    fig2, axs2 = plt.subplots(tot)\n",
    "    fig2.suptitle(DB_title + \" Freqs: \" + str(rand_ind))\n",
    "    for j in range(tot):\n",
    "        Four, freqs = four_tr(myTraces, int(rand_ind[j]), f_s)\n",
    "    #     fig, ax = plt.subplots()\n",
    "        axs2[-j].stem(freqs, np.abs(Four), use_line_collection=True, linefmt='-', markerfmt=\" \")\n",
    "    #     axs2[j].set_xlim(-f_s / 2, f_s / 2)\n",
    "    #     axs2[j].set_ylim(-5, 110)\n",
    "\n",
    "    # zoomed in frequency plot\n",
    "    # print(\"++  plot frequencies (zoomed in)\")\n",
    "    # fig3, axs3 = plt.subplots(tot)\n",
    "    # fig3.suptitle(DB_title + \" Freqs: \" + str(rand_ind))\n",
    "    # for k in range(tot):\n",
    "    #     Four, freqs = four_tr(myTraces, int(rand_ind[k]), f_s)\n",
    "    #     Four[0] = 0\n",
    "    #     axs3[k].stem(freqs, np.abs(Four), use_line_collection=True, linefmt='-', markerfmt=\" \")\n",
    "    #     axs3[k].set_xlim(-10, 10)\n",
    "    #     axs3[k].set_ylim(-1, 10)\n",
    "\n",
    "    \n",
    "    \n",
    "    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.spectrogram.html\n",
    "    print(\"++ plot spectrogram\")\n",
    "    fig4, axs4 = plt.subplots(tot)\n",
    "    fig4.suptitle(DB_title + \" sprectrogram: \" + str(rand_ind))\n",
    "    fig4.axes[-1].set_xlabel('time (sec)')\n",
    "    fig4.axes[int(tot/2)].set_ylabel('frequency')\n",
    "\n",
    "    for l in range(tot):\n",
    "        f, t, Sxx = signal.spectrogram(myTraces[int(rand_ind[l])], f_s)\n",
    "        axs4[l].pcolormesh(t, f, Sxx)\n",
    "#         axs4[l].set_xlim(-1, 20)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASCAD: architecture code, adapted by Mahmoud Gharra and fitted with new hyper parameters and optimizer.\n",
    "\n",
    "def check_file_exists(file_path):\n",
    "    file_path = os.path.normpath(file_path)\n",
    "    if os.path.exists(file_path) == False:\n",
    "        print(\"Error: provided file path '%s' does not exist!\" % file_path)\n",
    "        sys.exit(-1)\n",
    "    return\n",
    "\n",
    "\n",
    "### MLP model\n",
    "def mlp(classes=3):\n",
    "\n",
    "    layer_nb = 4\n",
    "    node = 5\n",
    "    input_shape = SAMPLE_HIGH\n",
    "    model = Sequential()\n",
    "    # model.add(Dense(node, input_dim=1400, activation='relu'))\n",
    "    model.add(Dense(node, input_dim=input_shape, activation='relu'))\n",
    "    for i in range(layer_nb-2):\n",
    "        model.add(Dense(node, activation='relu'))\n",
    "    model.add(Dense(classes, activation='softmax'))\n",
    "    optimizer = RMSprop(lr=0.01)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "### CNN model - Took ASCAD base\n",
    "def cnn(classes=3):\n",
    "    # From VGG16 design\n",
    "    input_shape = (SAMPLE_HIGH,1)\n",
    "    img_input = Input(shape=input_shape)\n",
    "    # Block 1\n",
    "    x = Conv1D(4, 3, activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "    x = AveragePooling1D(2, strides=2, name='block1_pool')(x)\n",
    "    # Block 2\n",
    "    x = Conv1D(8, 3, activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    x = AveragePooling1D(2, strides=2, name='block2_pool')(x)\n",
    "    # Block 3\n",
    "    x = Conv1D(16, 3, activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    x = AveragePooling1D(2, strides=2, name='block3_pool')(x)\n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(5, activation='relu', name='fc1')(x)\n",
    "    x = Dense(5, activation='relu', name='fc2')(x)\n",
    "    x = Dense(classes, activation='softmax', name='predictions')(x)\n",
    "\n",
    "    inputs = img_input\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='cnn')\n",
    "    optimizer = Adam(lr=LEARNING_RATE)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "### CNN model 2 - ASCAD best cnn w/ adam\n",
    "def cnn2(classes=3):\n",
    "# From VGG16 design\n",
    "    input_shape = (SAMPLE_HIGH,1)\n",
    "    img_input = Input(shape=input_shape)\n",
    "    # Block 1\n",
    "    x = Conv1D(64, 11, activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "    x = AveragePooling1D(2, strides=2, name='block1_pool')(x)\n",
    "    # Block 2\n",
    "    x = Conv1D(128, 11, activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    x = AveragePooling1D(2, strides=2, name='block2_pool')(x)\n",
    "    # Block 3\n",
    "    x = Conv1D(256, 11, activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    x = AveragePooling1D(2, strides=2, name='block3_pool')(x)\n",
    "    # Block 4\n",
    "    x = Conv1D(512, 11, activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    x = AveragePooling1D(2, strides=2, name='block4_pool')(x)\n",
    "    # Block 5\n",
    "    x = Conv1D(512, 11, activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    x = AveragePooling1D(2, strides=2, name='block5_pool')(x)\n",
    "    # Classification block\n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "    x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "    x = Dense(classes, activation='softmax', name='predictions')(x)\n",
    "\n",
    "    inputs = img_input\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='cnn')\n",
    "    optimizer = RMSprop(lr=LEARNING_RATE) # usually 0.00001 for this one\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "### CNN model 2 - Simplified ASCAD\n",
    "def cnn3(classes=3):\n",
    "    # From VGG16 design\n",
    "    input_shape = (SAMPLE_HIGH,1)\n",
    "    img_input = Input(shape=input_shape)\n",
    "    # Block 0\n",
    "    x = BatchNormalization()(img_input)\n",
    "    x = GaussianNoise(0.01)(x)\n",
    "    # Block 1\n",
    "    x = Conv1D(8, 3, activation='relu', padding='valid', name='block1_conv1')(x)\n",
    "    x = MaxPooling1D(2, name='block1_pool')(x)\n",
    "    x = BatchNormalization(name='block1_bn')(x)\n",
    "    # Block 2\n",
    "    x = Conv1D(16, 3, activation='relu', padding='valid', name='block2_conv1')(x)\n",
    "    x = MaxPooling1D(2, name='block2_pool')(x)\n",
    "    # Block 3\n",
    "    x = Conv1D(32, 3, activation='relu', padding='valid', name='block3_conv1')(x)\n",
    "    x = MaxPooling1D(2, name='block3_pool')(x)\n",
    "    x = BatchNormalization(name='block3_bn')(x)\n",
    "    # Block 4\n",
    "    x = Conv1D(64, 3, activation='relu', padding='valid', name='block4_conv1')(x)\n",
    "    x = MaxPooling1D(2, name='block4_pool')(x)\n",
    "    # Block 5\n",
    "    x = Conv1D(64, 3, activation='relu', padding='valid', name='block5_conv1')(x)\n",
    "    x = MaxPooling1D(2, name='block5_pool')(x)\n",
    "    x = BatchNormalization(name='block5_bn')(x)\n",
    "    # Block 6\n",
    "    x = Conv1D(256, 3, activation='relu', padding='valid', name='block6_conv1')(x)\n",
    "    x = MaxPooling1D(2, name='block6_pool')(x)\n",
    "    x = Dropout(drop_out)(x)\n",
    "    # Classification block\n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(50, activation='relu', name='fc1')(x)\n",
    "    x = Dense(50, activation='relu', name='fc2')(x)\n",
    "    x = Dropout(drop_out)(x)\n",
    "    x = Dense(classes, activation='softmax', name='predictions')(x)\n",
    "\n",
    "    inputs = img_input\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='cnn')\n",
    "    optimizer = Adam(lr=LEARNING_RATE) # this one had 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONST = 0\n",
    "LOSS_CONST = 1\n",
    "ACC_CONST = 2\n",
    "ADV_CONST = 3\n",
    "TRN_GRPH_CONST = 4\n",
    "ADV_GRPH_CONST = 5\n",
    "TST_ACC_CONST = 6\n",
    "VAL_LOSS_CONST = 7\n",
    "VAL_ACC_CONST = 8\n",
    "VAL_ADV_CONST = 9\n",
    "TST_ADV_CONST = 10\n",
    "\n",
    "\n",
    "# fun little thing for naming saved files\n",
    "# ('/start_of_file_name', '.file_extension', '/file_folder') # The index comes from the constants above\n",
    "save_list = [('/mdl', '.h5', \"/model\"),\n",
    "             ('/trn_lss', '.npy', \"/trn_loss\"),\n",
    "             ('/trn_acc', '.npy', \"/trn_accuracy\"),\n",
    "             ('/trn_adv', '.npy', \"/trn_advantage\"),\n",
    "             ('/trn_grph', '.png', \"/graphs\"),\n",
    "             ('/adv_grph', '.png', \"\"),\n",
    "             ('/tst_acc', '.npy', \"\"),\n",
    "             ('/val_lss', '.npy', \"/val_loss\"),\n",
    "             ('/val_acc', '.npy', \"/val_accuracy\"),\n",
    "             ('/val_adv', '.npy', \"/val_advantage\"),\n",
    "             ('/tst_adv', '.npy', \"\")]\n",
    "\n",
    "\n",
    "def get_file_loc(save_loc, case):\n",
    "    loc = save_loc + save_list[case][2]\n",
    "    Path(loc).mkdir(parents=True, exist_ok=True)\n",
    "    return loc\n",
    "\n",
    "def get_file_path(save_loc, case, seed=None, key=None):\n",
    "    tmp_str = get_file_loc(save_loc, case) + save_list[case][0]\n",
    "    \n",
    "    if seed is not None:\n",
    "        tmp_str = tmp_str + \"_s{:04d}\".format(seed)\n",
    "    \n",
    "    if key is not None:\n",
    "        tmp_str = tmp_str + \"_k{:02d}\".format(key)\n",
    "    return tmp_str + \"{}\".format(save_list[case][1])\n",
    "\n",
    "def save_file(save_loc, file, case=-1, seed=None, key=None, att=None):\n",
    "    \n",
    "    file_path = get_file_path(save_loc, case, seed, key)\n",
    "    print(\"++ Saving: \", file_path)\n",
    "    \n",
    "\n",
    "    if case in [LOSS_CONST, ACC_CONST, VAL_LOSS_CONST, VAL_ACC_CONST, VAL_ADV_CONST]:\n",
    "        # This means that file is a numpy array\n",
    "        np.savetxt(file_path, file)\n",
    "    \n",
    "    elif case in [TST_ADV_CONST, TST_ACC_CONST]:\n",
    "        # This means that file is a dictionary\n",
    "        np.save(file_path, file)\n",
    "    \n",
    "    \n",
    "    elif case in [TRN_GRPH_CONST]:\n",
    "        # This means we want to save a graph of a singular key\n",
    "        # The graph needs the accuracy,  arrays\n",
    "        # File is history\n",
    "        plt.plot(file.history['accuracy'])\n",
    "        plt.plot(file.history['loss'])\n",
    "        if validation_split_const is not None:\n",
    "            plt.plot(file.history['val_accuracy'])\n",
    "            plt.plot(file.history['val_loss'])\n",
    "        tmp_title = 'Training Graph: ' + 'seed' +'{:02d}'.format(seed)\n",
    "        if key is not None:\n",
    "            tmp_title = tmp_title +\", key: \"+'{:02d}'.format(key)\n",
    "        if not att == None:\n",
    "            tmp_title = tmp_title + ', att: ' + '{:01d}'.format(att)\n",
    "        plt.title(tmp_title)\n",
    "        plt.ylabel('%')\n",
    "        plt.xlabel('Epoch')\n",
    "        if validation_split_const is not None:\n",
    "            plt.legend(['Acc', 'Valid. Acc', 'Loss', 'Valid. Loss'], loc='upper left')\n",
    "        else:\n",
    "            plt.legend(['Acc', 'Loss'], loc='upper left')\n",
    "        plt.savefig(file_path)\n",
    "        plt.clf()\n",
    "        \n",
    "    elif case in [ADV_GRPH_CONST]:\n",
    "        # This means we want to save a graph of a singular key\n",
    "        # The graph needs the accuracy,  arrays\n",
    "        # File is history\n",
    "        plt.plot(file.history['trn_advantage'])\n",
    "        if validation_split_const is not None:\n",
    "            plt.plot(file.history['val_advantage'])\n",
    "        \n",
    "        tmp_title = 'Advantage Graph: '\n",
    "        if seed is not None:\n",
    "            tmp_title = tmp_title + 'seed' +'{:02d}, '.format(seed)\n",
    "        if key is not None:\n",
    "            tmp_title = tmp_title +\"key: \"+'{:02d}, '.format(key)\n",
    "        if not att == None:\n",
    "            tmp_title = tmp_title + ', att: ' + '{:01d}'.format(att)\n",
    "            \n",
    "        plt.title(tmp_title)\n",
    "        plt.ylabel('%')\n",
    "        plt.xlabel('Epoch')\n",
    "        if validation_split_const is not None:\n",
    "            plt.legend(['trn. Adv', 'val. Adv'], loc='upper left')\n",
    "        else:\n",
    "            plt.legend(['trn. Adv'], loc='upper left')\n",
    "        plt.savefig(file_path)\n",
    "        plt.clf()\n",
    "    else:\n",
    "        raise ValueError(\"save_file was called with a wrong case\")\n",
    "        exit(-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ASCAD code adapted by Mahmoud Gharra to fit our purposes.\n",
    "\n",
    "\n",
    "## Training high level function \n",
    "def train_model(X_profiling, Y_profiling, model, save_loc, seed=0, key_idx=None, epochs=150, batch_size=100, validation_split=0.1):\n",
    "    for attempt in range(MAX_ATTEMPTS_PER_KEY):\n",
    "        model_dir = get_file_loc(save_loc, MODEL_CONST)\n",
    "        check_file_exists(os.path.dirname(model_dir))\n",
    "        # delete pre-existing weights\n",
    "        if (os.path.normpath(get_file_path(save_loc, MODEL_CONST, seed, key_idx)) == True):\n",
    "            os.remove(get_file_path(save_loc, MODEL_CONST, seed, key_idx))\n",
    "        # Save model every epoch\n",
    "        save_model = ModelCheckpoint(get_file_path(save_loc, MODEL_CONST, seed, key_idx))\n",
    "        callbacks = [save_model]\n",
    "\n",
    "        # Get the input layer shape\n",
    "        input_layer_shape = model.get_layer(index=0).input_shape\n",
    "        # Sanity check\n",
    "        if input_layer_shape[1] != len(X_profiling[0]):\n",
    "            print(\"Error: model input shape %d instead of %d is not expected ...\" % (input_layer_shape[1], len(X_profiling[0])))\n",
    "            sys.exit(-1)\n",
    "        # Adapt the data shape according our model input\n",
    "        if len(input_layer_shape) == 2:\n",
    "            # This is a MLP\n",
    "            Reshaped_X_profiling = X_profiling\n",
    "        elif len(input_layer_shape) == 3:\n",
    "            # This is a CNN: expand the dimensions\n",
    "            Reshaped_X_profiling = X_profiling.reshape((X_profiling.shape[0], X_profiling.shape[1], 1))\n",
    "        else:\n",
    "            print(\"Error: model input shape length %d is not expected ...\" % len(input_layer_shape))\n",
    "            sys.exit(-1)\n",
    "            \n",
    "        _history = training_model_intern(model=model, x=Reshaped_X_profiling, y=Y_profiling, callbacks=callbacks, batch_size=batch_size, verbose=1, epochs=epochs, validation_split=validation_split)\n",
    "        \n",
    "        if _history.history['accuracy'][-1] > MIN_ACC:\n",
    "            break\n",
    "        # getting here means we probably want to retry the training with new values\n",
    "    # TODO: instead of resetting _history after each attempt, add it to the final history\n",
    "    return _history, attempt+1\n",
    "\n",
    "\n",
    "def training_model_intern(model, x, y, callbacks, batch_size=100, verbose=1, epochs=150, validation_split=0.1):\n",
    "    if validation_split is not None:\n",
    "        _history = model.fit(x=x, y=y, batch_size=batch_size, verbose = verbose, epochs=epochs, callbacks=callbacks, validation_split=0.1)\n",
    "    else:\n",
    "        _history = model.fit(x=x, y=y, batch_size=batch_size, verbose = verbose, epochs=epochs, callbacks=callbacks)\n",
    "    return _history\n",
    "\n",
    "## Saves pre-defined history parameters that keras training returns. It assumes existence of validation data.\n",
    "def save_history(training_model, history, seed=None, key_idx=None, att=None, valid_val=None):\n",
    "    # SAVE HISTORY\n",
    "    ## SAVE HISTORY: LOSS per epoch\n",
    "    save_file(training_model, history.history['loss'], case=LOSS_CONST, seed=seed, key=key_idx)\n",
    "    ## SAVE HISTORY: ACCURACY per epoch\n",
    "    save_file(training_model, history.history['accuracy'], case=ACC_CONST, seed=seed, key=key_idx)\n",
    "    if valid_val is not None:\n",
    "        ## SAVE HISTORY: VAL_LOSS per epoch\n",
    "        save_file(training_model, history.history['val_loss'], case=VAL_LOSS_CONST, seed=seed, key=key_idx)\n",
    "        ## SAVE HISTORY: VAL_ACCURACY per epoch\n",
    "        save_file(training_model, history.history['val_accuracy'], case=VAL_ACC_CONST, seed=seed, key=key_idx)\n",
    "\n",
    "    ## SAVE HISTORY: GRAPH\n",
    "    save_file(training_model, history, case=TRN_GRPH_CONST, seed=seed, key=key_idx, att=att)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method was taken from the ASCAD code and adapted very heavily\n",
    "\n",
    "def calc_tst_acc(best_model, X_attack, Y_attack, num_classes=3, seed=None, key_idx=None):\n",
    "    \n",
    "    if key_idx is not None:\n",
    "        tmp_output = \"++ calculating accuracy for seed {} and key {}\".format(seed,key_idx)\n",
    "    else:\n",
    "        tmp_output =  \"++ calculating accuracy for seed {}\".format(seed)\n",
    "    print(tmp_output)\n",
    "    \n",
    "    # Get the input layer shape\n",
    "    input_layer_shape = best_model.get_layer(index=0).input_shape\n",
    "    # Sanity check\n",
    "    if input_layer_shape[1] != len(X_profiling[0]):\n",
    "        print(\"Error: model input shape %d instead of %d is not expected ...\" % (input_layer_shape[1], len(X_profiling[0])))\n",
    "        sys.exit(-1)\n",
    "    # Adapt the data shape according our model input\n",
    "    if len(input_layer_shape) == 2:\n",
    "        # This is a MLP\n",
    "        Reshaped_X_attack = X_attack\n",
    "    elif len(input_layer_shape) == 3:\n",
    "        # This is a CNN: expand the dimensions\n",
    "        Reshaped_X_attack = X_attack.reshape((X_attack.shape[0], X_attack.shape[1], 1))\n",
    "    else:\n",
    "        print(\"Error: model input shape length %d is not expected ...\" % len(input_layer_shape))\n",
    "        sys.exit(-1)\n",
    "    \n",
    "    predictions = np.argmax(best_model.predict(Reshaped_X_attack), 1)\n",
    "#     print(\"predictions: \", predictions)\n",
    "#     print(\"Y_profiling: \", Y_attack)\n",
    "#     print(\"predictions cmp Y_profiling: \", (Y_attack == predictions))\n",
    "    \n",
    "    accuracy = sum(Y_attack == predictions)/TEST_NUM \n",
    "    return accuracy\n",
    "\n",
    "def calc_advantage(tst_acc):\n",
    "    return (tst_acc - .62) / (1-.62)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load attacking traces\n",
    "# TODO: fix this method up and make it work for ASCAD too.\n",
    "def display_results(models, accuracies, advantage, isASCAD=False):\n",
    "    \n",
    "    if not isASCAD:\n",
    "    #     # we get 61% accuracy by just guessing, so let's compute the advantage over pure guesses:\n",
    "    #     print(\"+ recording calculating advantage\")\n",
    "    #     advantage = { i: (seed, key_idx, (accuracies[(seed, key_idx)] - .62) / (1-.62)) for i, (seed, key_idx) in enumerate(models.keys())}\n",
    "    #     # data = DataFrame.from_dict(advantage, orient='index', columns=['seed', 'key_idx', 'advantage'])\n",
    "    #     print(\"+ displaying data\")\n",
    "    #     # catplot(data=data, x=\"key_idx\", y=\"advantage\", row=\"seed\", kind=\"bar\")\n",
    "    #     print(\"advantage\", advantage)\n",
    "        adv = { i: (seed, key_idx, (advantage[(seed, key_idx)])) for i, (seed, key_idx) in enumerate(models.keys())}\n",
    "        print(\"adv: \", adv)\n",
    "        data = DataFrame.from_dict(adv, orient='index', columns=['seed', 'key_idx', 'adv'])\n",
    "        catplot(data=data, x=\"key_idx\", y=\"adv\", row=\"seed\", kind=\"bar\")\n",
    "\n",
    "        plt.savefig(get_file_path(training_model, ADV_GRPH_CONST))\n",
    "\n",
    "\n",
    "    # # # TODO: Needs to be moved to save_file(). You also need to call it before display results but after end of training\n",
    "    # # def save_adv(accuracy, save_location, seed, key_idx):\n",
    "    # #     advantage = (accuracies[(seed, key_idx)] - .62) / (1-.62)\n",
    "    # #     file_path = save_location + '/advantage' + '{:02d}'.format(seed)+\"key\"+'{:02d}'.format(key_idx)+\".txt\"\n",
    "    # #     np.savetxt(, advantage)\n",
    "    # #     return advantage\n",
    "    else:\n",
    "        adv = { i: (seed, (advantage[(seed)])) for i, (seed) in enumerate(models.keys())}\n",
    "        print(\"adv: \", adv)\n",
    "        data = DataFrame.from_dict(adv, orient='index', columns=['seed', 'adv'])\n",
    "        catplot(data=data, y=\"adv\", x=\"seed\", kind=\"bar\")\n",
    "        plt.savefig(get_file_path(training_model, ADV_GRPH_CONST))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare\n",
    "models = dict()\n",
    "\n",
    "# define empty history object\n",
    "history = dict()\n",
    "\n",
    "# # data types:\n",
    "# ASCAD_DB = 0\n",
    "# CHIP_WHISP_DB = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ASCAD: Adapted heavily by Mahmoud Gharra\n",
    "\n",
    "# NOTE: code could take another database if it has the same format as the one provided.\n",
    "# NOTE: you need to make a local directory with the name of your chosen training_model\n",
    "\n",
    "############################################################\n",
    "############################################################\n",
    "# 1st set of Hyper parameters: #############################\n",
    "############################################################\n",
    "# Visualization and data sampling\n",
    "tot = 0 # total number of random samples that you'd like to display\n",
    "f_s = 100 # frequency of data in Hz --- I can't seem to find this value in proj\n",
    "\n",
    "# Training\n",
    "tst_len = 500 # length of testing set - How many traces would you like to allocate to training?\n",
    "my_seeds = [634253, 9134, 57935] # training seeds - list of seed for network to be trained on. Useful for replicating similar results.\n",
    "\n",
    "#################################################\n",
    "#################################################\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "if len(sys.argv)!=2:\n",
    "    ############################################################\n",
    "    ############################################################\n",
    "    # 2nd set of Hyper parameters:\n",
    "    ############################################################\n",
    "    #default parameters values\n",
    "#     my_database = \"../2020_APR_23/polymul32/projects/operand_scanning_32\" # Loc on my personal pc\n",
    "#     my_database = \"../chipWhisp01/projects/operand_scanning_32\" # Loc on Einstein\n",
    "#     DB_title = \"operand_scanning_32\" # arbitrary name\n",
    "#     isASCAD = False\n",
    "\n",
    "\n",
    "    DB_title = my_database = \"schoolbook32/schoolbook32\" # loc on HPC      \n",
    "    DB_title = \"schoolbook32\" ## Optional... It's for the graph\n",
    "    isASCAD = False\n",
    "\n",
    "\n",
    "#     my_database = \"../2020_MAR_31/ASCAD_data/ASCAD_databases/ASCAD.h5\" # Loc on my personal pc\n",
    "#     my_database = \"../PRE_MAY_06/ASCAD/ATMEGA_AES_v1/ATM_AES_v1_fixed_key/ASCAD_data/ASCAD_databases/ASCAD.h5\" # Loc on Einstein\n",
    "#     DB_title = \"ASCAD\"\n",
    "#     isASCAD = True\n",
    "\n",
    "    # TRAINING MODEL IS THE FILE, IN WHICH THE DATA SHOULD BE SAVED\n",
    "    # Network type simply chooses the architecture according to which the data is trained\n",
    "    \n",
    "    # 'cnn' works well for operand_scanning_32 with 20 epochs batch 100 and 2 attempts\n",
    "        \n",
    "    # CNN training\n",
    "    network_type = \"cnn\" ## ATM: you can choose between 'mlp', 'cnn', 'cnn2', and 'cnn3'\n",
    "    # save folder\n",
    "    training_model = \"training_ASCAD_cnn_batch100_epochs2_MAXATT1_lr1e-5_valid1e-1\"\n",
    "    \n",
    "    epochs = 200\n",
    "    batch_size = 100\n",
    "    global MAX_ATTEMPTS_PER_KEY, drop_out, MIN_ACC\n",
    "    MIN_ACC = 0.95\n",
    "#     EARLY_STOP_PATIENCE = 50\n",
    "    drop_out = 0.2\n",
    "    MAX_ATTEMPTS_PER_KEY = 1\n",
    "    LEARNING_RATE = 0.00001\n",
    "    validation_split_const = 0.1 # (None or a float in }0,1{)\n",
    "    ############################################################\n",
    "    # Don't change anything from this point on,\n",
    "    # unless you have an intuition for how the code works\n",
    "    ############################################################\n",
    "else:\n",
    "    #get parameters from user input\n",
    "    DB_title, database, training_model, network_type, epochs, batch_size= read_parameters_from_file(sys.argv[1])\n",
    "\n",
    "# create save directory, if it doesn't exist\n",
    "Path(training_model).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "\n",
    "accuracies = {}\n",
    "advantages = {}\n",
    "### training\n",
    "print(\"+ Commense training (takes about 20 seconds per epoch)\")\n",
    "for seed in my_seeds:\n",
    "    # increasing reproducability of results according to https://keras.io/getting_started/faq/\n",
    "    # set PYTHONHASHSEED before running program to increase reproducability\n",
    "    # You can also consider using the CPU, as GPUs are less deterministic. (CUDA=\"\")\n",
    "    np.random.seed(seed)\n",
    "    python_random.seed(seed)\n",
    "    tf.random.set_random_seed(seed) # older tf versions use this\n",
    "#     tf.random.set_seed(seed)\n",
    "\n",
    "    # Train if the data_base is of type .h5 and has the structure used in ASCAD\n",
    "    if isASCAD: # Change for DB_Type at some point\n",
    "        # loads ASCAD traces\n",
    "        print(\"+ Commense loading data\")\n",
    "        (X_profiling, Y_profiling), (X_attack, Y_attack) = load_ascad(my_database)\n",
    "        global SAMPLE_HIGH\n",
    "        # Organize trace data for MLP\n",
    "        print(\"++ Organizing traces\")\n",
    "        SAMPLE_HIGH = X_profiling.shape[1]\n",
    "        TEST_NUM = X_attack.shape[1]\n",
    "        \n",
    "        print(\"++ Profiling traces dimestions are: \", X_profiling.shape)\n",
    "        # sample some traces\n",
    "        print(\"+ sample {0} profiling traces\".format(tot))\n",
    "        if tot != 0:\n",
    "            sample_traces(X_profiling)\n",
    "            \n",
    "        #get network type\n",
    "        if(network_type==\"mlp\"):\n",
    "            best_model = mlp(256)\n",
    "        elif(network_type==\"cnn\"):\n",
    "            best_model = cnn(256)\n",
    "        elif(network_type==\"cnn2\"):\n",
    "            best_model = cnn2(256)\n",
    "        elif(network_type==\"cnn3\"):\n",
    "            best_model = cnn3(256)\n",
    "        else: #display an error and abort\n",
    "            print(\"Error: no topology found for network '%s' ...\" % network_type)\n",
    "            sys.exit(-1);\n",
    "\n",
    "        ### training\n",
    "        history, att = train_model(X_profiling, to_categorical(Y_profiling, num_classes=256), best_model, training_model, epochs = epochs, batch_size=batch_size, seed=seed, validation_split=validation_split_const)\n",
    "\n",
    "        # SAVE HISTORY\n",
    "        save_history(training_model, history, seed=seed, att=att)\n",
    "        \n",
    "        # add file path to list of trained models\n",
    "        models[seed] = get_file_path(training_model, MODEL_CONST, seed=seed) # TODO: models can be removed, get_file_path() is better at getting model instance\n",
    "        # TODO: THE FOLLOWING 4 LINES WERE REPEATED 4 TIMES\n",
    "        ## calculate test accuracy\n",
    "        accuracies[seed] = calc_tst_acc(best_model, X_attack, Y_attack, seed=seed, num_classes=256)\n",
    "        ## calculate ADVANTAGE\n",
    "        advantages[seed] = calc_advantage(accuracies.get(seed))\n",
    "\n",
    "    # Train if we're dealing with chip whisperer readings\n",
    "    else:\n",
    "        # loads cw traces\n",
    "        print(\"+ Commense loading data\")\n",
    "        X, y = load_database_cw(my_database)\n",
    "        print(\"++ Data dimestions are: \", np.array(X).shape)\n",
    "\n",
    "        # sample some traces\n",
    "        # Sampling traces intereferes with the resulting graph. This must mean that I'm doing that suboptimally\n",
    "        # TODO: fix it, so that saved graph is correct\n",
    "        print(\"+ sample {0} traces\".format(tot))\n",
    "        if tot != 0:\n",
    "            sample_traces(X)\n",
    "            print(\"+ print out list of categories:\\n {}\".format(np.unique(y)))\n",
    "            print(\"+ print out shape of labels:\\n {}\".format(y.shape))\n",
    "        \n",
    "        for key_idx in range(KEY_LENGTH):\n",
    "            print('key_idx={}...'.format(key_idx))\n",
    "\n",
    "            #get network type\n",
    "            if(network_type==\"cnn\"):\n",
    "                best_model = cnn()\n",
    "            elif(network_type==\"cnn2\"):\n",
    "                best_model = cnn2()\n",
    "            elif(network_type==\"cnn3\"):\n",
    "                best_model = cnn3()\n",
    "            elif(network_type==\"mlp\"):\n",
    "                best_model = mlp()\n",
    "            else: #display an error and abort\n",
    "                print(\"Error: no topology found for network '%s' ...\" % network_type)\n",
    "                sys.exit(-1);\n",
    "\n",
    "            # extract prof and testing data\n",
    "            (X_profiling, Y_profiling), (X_testing, Y_testing) = extract_data(X, y, key_idx, best_model)\n",
    "\n",
    "\n",
    "            # we run individual training for each key integer\n",
    "            if models.get((seed, key_idx)) is not None:\n",
    "                ## calculate ACCURACY\n",
    "                accuracies[(seed, key_idx)] = calc_tst_acc(best_model, X_testing, Y_testing, seed=seed, key_idx=key_idx)\n",
    "                ## calculate ADVANTAGE\n",
    "                advantages[(seed, key_idx)] = calc_advantage(accuracies.get((seed, key_idx)))\n",
    "                continue\n",
    "\n",
    "            _file_path = os.path.normpath(get_file_path(training_model, MODEL_CONST, seed, key_idx))\n",
    "            if os.path.exists(_file_path) == True:\n",
    "                models[(seed, key_idx)] = get_file_path(training_model, MODEL_CONST, seed, key_idx)\n",
    "                ## calculate ACCURACY\n",
    "                accuracies[(seed, key_idx)] = calc_tst_acc(best_model, X_testing, Y_testing, seed=seed, key_idx=key_idx)\n",
    "                ## calculate ADVANTAGE\n",
    "                advantages[(seed, key_idx)] = calc_advantage(accuracies.get((seed, key_idx)))\n",
    "                continue                \n",
    "\n",
    "            history, att = train_model(X_profiling, to_categorical(Y_profiling, num_classes=3), best_model, training_model, seed=seed, key_idx=key_idx, epochs=epochs, batch_size=batch_size)\n",
    "            \n",
    "            ## TODO: save hist\n",
    "            print(\"+ save history\")\n",
    "            # SAVE HISTORY\n",
    "            save_history(training_model, history, seed, key_idx, att)\n",
    "\n",
    "            # add file path to list of trained models\n",
    "            models[(seed, key_idx)] = get_file_path(training_model, MODEL_CONST, seed, key_idx) # TODO: models can be removed, get_file_path() is better at getting model instance\n",
    "            # TODO: THE FOLLOWING 4 LINES WERE REPEATED 4 TIMES\n",
    "            ## calculate test accuracy, you'll be saving them all at the end\n",
    "            accuracies[(seed, key_idx)] = calc_tst_acc(best_model, X_testing, Y_testing, seed=seed, key_idx=key_idx)\n",
    "            \n",
    "            ## calculate ADVANTAGE\n",
    "            advantages[(seed, key_idx)] = calc_advantage(accuracies.get((seed, key_idx)))\n",
    "\n",
    "print()\n",
    "print(\"+ Training complete\")\n",
    "\n",
    "# Save all test accuracies and advantages\n",
    "print(\"+ Saving dicctionary with all test accuracies and test advantages of all seeds (and keys if those exist)\")\n",
    "save_file(training_model, accuracies, case=TST_ACC_CONST)\n",
    "save_file(training_model, advantages, case=TST_ADV_CONST)\n",
    "\n",
    "\n",
    "\n",
    "# TODO: replace calculation of advantage with loading of it\n",
    "display_results(models, accuracies, advantages, isASCAD=isASCAD)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
