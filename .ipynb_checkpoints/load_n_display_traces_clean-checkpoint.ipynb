{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code simply loads traces and displays them in a spectrogram and in a plot\n",
    "# It does not contain any training functions or parameters\n",
    "# It was created to increase the readability of the main code and to enable the display of the loaded traces (Duh :P)\n",
    "\n",
    "\n",
    "# This code contains code adapted from Nils Wisiol, sntrup4591761 and ANSSI-FR/ASCAD\n",
    "# All snippets are marked accordingly as such\n",
    "# from tqdm import tnrange\n",
    "\n",
    "# General modules\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Load data modules\n",
    "from numpy import empty, zeros, uint16\n",
    "from numpy.random import RandomState\n",
    "import chipwhisperer as cw\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "## Load data modules for DPA contest\n",
    "# This library is from another GitHub library: https://github.com/yetifrisstlama/readTrc\n",
    "# from readTrc import trc\n",
    "from readTrc import Trc\n",
    "\n",
    "# Plot traces\n",
    "from scipy import signal, fftpack\n",
    "import matplotlib.pyplot as plt\n",
    "## Plot zoomed in\n",
    "%matplotlib inline\n",
    "from ipywidgets import interactive\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASCAD: Adapted by Mahmoud Gharra to fit the NTRU Prime input\n",
    "\n",
    "\n",
    "# returns traces and labels, as well as some important global meta_data\n",
    "\n",
    "def load_database(my_database):\n",
    "    # load traces\n",
    "    print(\"++ Loading projects\")\n",
    "    project = cw.open_project(my_database)\n",
    "    global KEY_LENGTH, TEXT_LENGTH, SAMPLE_HIGH, TRAINING_SLICE, TEST_NUM, TEST_SLICE\n",
    "\n",
    "    # Organize trace data for MLP\n",
    "    print(\"++ Organizing traces\")\n",
    "    KEY_LENGTH = TEXT_LENGTH = len(project.keys[0]) * 8 # TODO: reset\n",
    "#     KEY_LENGTH = TEXT_LENGTH = len(project.keys[0]) # TODO: reset\n",
    "    print(\"KEY_LENGTH: {}\".format(KEY_LENGTH))\n",
    "    print(\"project.keys[0][0]: {}\".format(project.keys[0][0]))\n",
    "    print(\"np.asarray(bytearray(project.keys[0]))\", np.asarray(bytearray(project.keys[0])))\n",
    "\n",
    "    # SET DATA RELATED GLOBALS REQUIRED FOR EXTRACTION\n",
    "    sample_low = 0\n",
    "    SAMPLE_HIGH = project.trace_manager().num_points() # length of singular trace\n",
    "    sample_slice = slice(sample_low, SAMPLE_HIGH)\n",
    "    sample_num = len(project.traces) # number of traces\n",
    "#     print(\"sample num: \", sample_num)\n",
    "    \n",
    "    # organize traces in X matrix\n",
    "    X = empty(shape=(sample_num, SAMPLE_HIGH - sample_low))\n",
    "\n",
    "    # organize the operands in Y matrix\n",
    "    count = 0\n",
    "    \n",
    "    y = empty(shape=(sample_num, KEY_LENGTH))\n",
    "    for i in range(sample_num):\n",
    "        # reproduce values\n",
    "        \n",
    "        # The next three lines were for debugging purposes. I'm sorry for doing printf debugging :p\n",
    "#         print(\"key of sample {}: {}\".format(i, project.keys[i]))\n",
    "#         print(\"data type of key of sample {}: {}\".format(i, type(project.keys[i])))\n",
    "#         print(\"ISO-8859-1 encoding: {}\".format(project.keys[i].decode(\"ISO-8859-1\")))\n",
    "\n",
    "        if (len(np.asarray(bytearray(project.keys[i])))) is not (int(KEY_LENGTH/8)):\n",
    "            count += 1\n",
    "#             print(\"Number of problematic traces raised to: {}\".format(count))\n",
    "#             print(\"actual key len: {}, expected length: {}\".format(len(np.asarray(bytearray(project.keys[i]))), int(KEY_LENGTH/8)))\n",
    "            continue\n",
    "            \n",
    "#         print(\"y[{}]: {}\".format(i-count, np.asarray(bytearray(project.keys[i]))))\n",
    "#         print(\"np.asarray(bytearray(project.keys[i])): {}\".format(len(np.asarray(bytearray(project.keys[i])))))\n",
    "        y[i-count] = np.asarray([int(char) for char in ''.join(['{0:08b}'.format(ff) for ff in np.asarray(bytearray(project.keys[0]))])])\n",
    "#         y[i-count] = np.asarray(bytearray(project.keys[i]))\n",
    "        X[i-count] = project.waves[i][sample_slice]\n",
    "\n",
    "#     remove last {count} rows for having wrong KEY-Dimesions\n",
    "    if i is not 0:\n",
    "        y = y[0:-count]\n",
    "        X = X[0:-count]\n",
    "\n",
    "#     # transform generated key numbers to labels\n",
    "#     y = np.array((y.T[1::2].T*(2**8)) + y.T[::2].T, dtype=np.int16)\n",
    "\n",
    "    # the next 4 lines aren't needed because the labels for the sampler are ints starting at 0\n",
    "#     unique = np.unique(y)\n",
    "#     class_dic = dict([[unique[i], i] for i in range(len(unique))])\n",
    "#     y_labelled = np.vectorize(class_dic.get)(y)\n",
    "#     return X, y_labelled\n",
    "\n",
    "\n",
    "    # SET DATA RELATED GLOBALS before returning (POST EXTRACTION)\n",
    "    sample_low = 0\n",
    "    SAMPLE_HIGH = project.trace_manager().num_points() # length of singular trace\n",
    "    sample_slice = slice(sample_low, SAMPLE_HIGH)\n",
    "    sample_num = len(project.traces) - count # number of traces\n",
    "    \n",
    "    training_num = sample_num - tst_len\n",
    "    TRAINING_SLICE = slice(0, training_num)\n",
    "\n",
    "    TEST_NUM = sample_num - training_num\n",
    "    TEST_SLICE = slice(training_num, TEST_NUM + training_num)\n",
    "    assert TEST_NUM + training_num <= sample_num\n",
    "    assert training_num > 3*TEST_NUM\n",
    "\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def read_parameters_from_file(param_filename):\n",
    "    #read parameters for the train_model and load_traces functions\n",
    "    #TODO: sanity checks on parameters\n",
    "    param_file = open(param_filename,\"r\")\n",
    "\n",
    "    #TODO: replace eval() by ast.linear_eval()\n",
    "    my_parameters= eval(param_file.read())\n",
    "\n",
    "    my_database = my_parameters[\"database\"]\n",
    "    my_database_title = my_parameters[\"database_title\"]\n",
    "    return my_database_title, my_database\n",
    "\n",
    "\n",
    "\n",
    "# https://www.oreilly.com/library/view/elegant-scipy/9781491922927/ch04.html\n",
    "def four_tr(traces, tmp_i, f_s):\n",
    "    _Four = fftpack.fft(traces[tmp_i])\n",
    "    _freqs = fftpack.fftfreq(len(traces[tmp_i])) * f_s\n",
    "    \n",
    "    return _Four, _freqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASCAD: Adapted by Mahmoud Gharra to fit the NTRU Prime input\n",
    "\n",
    "\n",
    "# returns traces and labels, as well as some important global meta_data\n",
    "\n",
    "def load_database_DPA(my_database_traces, my_database_labels):\n",
    "    # load traces\n",
    "    \n",
    "        \n",
    "    global KEY_LENGTH, TEXT_LENGTH, SAMPLE_HIGH, TRAINING_SLICE, TEST_NUM, TEST_SLICE\n",
    "\n",
    "    # Organize trace data for MLP\n",
    "    print(\"++ Organizing traces\")\n",
    "    KEY_LENGTH = TEXT_LENGTH = 1\n",
    "    \n",
    "    print(\"KEY_LENGTH: {}\".format(KEY_LENGTH))\n",
    "\n",
    "    # get the first trace to get its dims\n",
    "    trc = Trc()\n",
    "    datX, datY, d = trc.open(my_database_traces + \"/Z1Trace00000.trc\")\n",
    "    # SET DATA RELATED GLOBALS REQUIRED FOR EXTRACTION\n",
    "    sample_low = 0\n",
    "    SAMPLE_HIGH = len(datX)/32 # length of singular trace\n",
    "    sample_slice = slice(sample_low, SAMPLE_HIGH)\n",
    "    sample_num = 2000 # number of traces\n",
    "#     print(\"sample num: \", sample_num)\n",
    "    \n",
    "    print(\"++ Loading projects\")\n",
    "\n",
    "    \n",
    "    # organize traces in X matrix\n",
    "    X = empty(shape=(sample_num, int(SAMPLE_HIGH) - sample_low + 1))\n",
    "    \n",
    "    # organize the operands in Y matrix\n",
    "    y = empty(shape=(sample_num, KEY_LENGTH))\n",
    "    \n",
    "    f = open (my_database_labels , 'r')\n",
    "    fs = []\n",
    "    fs = [i for i in f]\n",
    "\n",
    "    for i in range(sample_num):\n",
    "        datX, datY, d = trc.open(my_database_traces + \"/Z1Trace{:05d}.trc\".format(i))\n",
    "        X[i] = datY[0:int(len(datX)/4):8]\n",
    "        y[i] = int(fs[i].split()[3], 16)\n",
    "\n",
    "\n",
    "    # SET DATA RELATED GLOBALS before returning (POST EXTRACTION)\n",
    "    sample_low = 0\n",
    "    SAMPLE_HIGH = int(len(datX)/32) # length of singular trace\n",
    "    sample_slice = slice(sample_low, SAMPLE_HIGH)\n",
    "    sample_num = 500 # number of traces\n",
    "    \n",
    "    training_num = sample_num - tst_len\n",
    "    TRAINING_SLICE = slice(0, training_num)\n",
    "\n",
    "    TEST_NUM = sample_num - training_num\n",
    "    TEST_SLICE = slice(training_num, TEST_NUM + training_num)\n",
    "    assert TEST_NUM + training_num <= sample_num\n",
    "    assert training_num > 3*TEST_NUM\n",
    "\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these functions were used in the loading of m4sc data\n",
    "def bit8(i):\n",
    "    return [(0 if (0 == (i & (1 << j))) else 1) for j in range(8)]\n",
    "\n",
    "\n",
    "def load_database_pickle_normalized(my_database):\n",
    "\n",
    "        # load traces\n",
    "        print(\"++ Loading schoolbook on m4sc data\")\n",
    "\n",
    "        data = []\n",
    "        data_file = my_database\n",
    "        with open(data_file, 'rb') as f:\n",
    "            while True:\n",
    "                try:\n",
    "                    data.append(pickle.load(f))\n",
    "                except EOFError:\n",
    "                    break\n",
    "\n",
    "        # parse traces - convert int array to bit array\n",
    "        print(\"++ Sample Pickle data\")\n",
    "        print(\"len(data): {}\".format(len(data)))\n",
    "        print(\"data[0]: {}\".format(data[0]))\n",
    "        \n",
    "        \n",
    "        print(\"++ Parse m4sc data\")\n",
    "        X = np.array([i[4] for i in data])\n",
    "        y_init = np.array([int((int(i[0][-2:], 16))) for i in data])\n",
    "        y = np.array([bit8(i) for i in y_init], dtype=np.uint8)\n",
    "        print(\"++ Finished parsing m4sc data\")\n",
    "\n",
    "        print(X.shape)\n",
    "        print(y.shape)\n",
    "        print(y)\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"++ Normalizing traces of m4sc\")\n",
    "        X = X - X.mean()\n",
    "        X = X / X.max()\n",
    "\n",
    "\n",
    "        # Organize trace data for network\n",
    "        print(\"++ Organizing traces\")\n",
    "    #     KEY_LENGTH = TEXT_LENGTH = 1\n",
    "        KEY_LENGTH = 1\n",
    "#         print(\"KEY_LENGTH: {}\".format(self.KEY_LENGTH))\n",
    "\n",
    "        # SET DATA RELATED GLOBALS REQUIRED FOR EXTRACTION\n",
    "        sample_len = X.shape[1] # length of singular trace\n",
    "        trace_num = X.shape[0] # number of traces\n",
    "\n",
    "        self.__set_dims(sample_len, trace_num)\n",
    "        \n",
    "                          \n",
    "        return X, y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_traces(unlab_traces):\n",
    "    \n",
    "    t = np.arange(unlab_traces.shape[1])\n",
    "    myTraces = np.array(unlab_traces)\n",
    "\n",
    "    rand_ind = np.zeros(tot)\n",
    "    for i in range(tot):\n",
    "        rand_ind[i] = np.random.randint(myTraces.shape[0])\n",
    "\n",
    "    # plot traces\n",
    "    print(\"++ plot traces\")\n",
    "    fig1, axs1 = plt.subplots(tot)\n",
    "    fig1.suptitle(DB_title + \": \" + str(rand_ind))\n",
    "    for i in range(tot):\n",
    "        axs1[i].plot(t, myTraces[int(rand_ind[i])])\n",
    "\n",
    "    # zoomed in traces plot    \n",
    "#     print(\"++ plot zoomed in traces\")\n",
    "#     fig5, axs5 = plt.subplots(2)\n",
    "#     fig5.suptitle(DB_title + \": \" + str(rand_ind))\n",
    "    \n",
    "#     for m in range(2):\n",
    "#         def f(m):\n",
    "#             axs5[m].plot(t, myTraces[int(rand_ind[m])])\n",
    "#             axs5[m].set_xlim(m, m+1000)\n",
    "#         interactive(f, m=(0, 900.0))\n",
    "        \n",
    "    \n",
    "    \n",
    "    # https://www.oreilly.com/library/view/elegant-scipy/9781491922927/ch04.html    \n",
    "    # frequency plot\n",
    "    print(\"++ plot frequencies\")\n",
    "    fig2, axs2 = plt.subplots(tot)\n",
    "    fig2.suptitle(DB_title + \" Freqs: \" + str(rand_ind))\n",
    "    for j in range(tot):\n",
    "        Four, freqs = four_tr(myTraces, int(rand_ind[j]), f_s)\n",
    "    #     fig, ax = plt.subplots()\n",
    "        axs2[-j].stem(freqs, np.abs(Four), use_line_collection=True, linefmt='-', markerfmt=\" \")\n",
    "        \n",
    "    #     axs2[j].set_xlim(-f_s / 2, f_s / 2)\n",
    "    #     axs2[j].set_ylim(-5, 110)\n",
    "\n",
    "    # zoomed in frequency plot\n",
    "#     print(\"++  plot frequencies (zoomed in)\")\n",
    "#     fig3, axs3 = plt.subplots(tot)\n",
    "#     fig3.suptitle(DB_title + \" Freqs: \" + str(rand_ind))\n",
    "#     for k in range(tot):\n",
    "#         Four, freqs = four_tr(myTraces, int(rand_ind[k]), f_s)\n",
    "#         Four[0] = 0\n",
    "#         axs3[k].stem(freqs, np.abs(Four), use_line_collection=True, linefmt='-', markerfmt=\" \")\n",
    "#         axs3[k].set_xlim(-10, 10)\n",
    "#         axs3[k].set_ylim(-1, 10)\n",
    "\n",
    "    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.spectrogram.html\n",
    "#     print(\"++ plot spectrogram\")\n",
    "#     fig4, axs4 = plt.subplots(tot)\n",
    "#     fig4.suptitle(DB_title + \" sprectrogram: \" + str(rand_ind))\n",
    "#     fig4.axes[-1].set_xlabel('time (sec)')\n",
    "#     fig4.axes[int(tot/2)].set_ylabel('frequency')\n",
    "\n",
    "#     for l in range(tot):\n",
    "#         f, t, Sxx = signal.spectrogram(myTraces[int(rand_ind[l])], f_s)\n",
    "#         axs4[l].pcolormesh(t, f, Sxx)\n",
    "# #         axs4[l].set_xlim(-1, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_db(db_name, traces, labels):\n",
    "    np.save(\"../dpa/traces.npy\", traces)\n",
    "    np.save(\"../dpa/labels.npy\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Commense loading data\n",
      "++ Loading schoolbook on m4sc data\n",
      "++ Sample Pickle data\n",
      "len(data): 10000\n",
      "data[0]: ['856803817c1b6716d20fc133d5a08c2b03827d917ec2897d', 'a3334b616ff85f616d9d4a6884819982176eefdd4a5ac53a', [2991, 49632, 19584, 55441, 40805, 19663, 61615, 23524, 17349, 4544, 20431, 55781, 18242, 21588, 63264, 50643, 36695, 37211, 53376, 14690, 17284, 43664, 42093], False, array([ 0.00488281, -0.01367188, -0.00488281, ..., -0.01269531,\n",
      "       -0.00585938, -0.00390625])]\n",
      "++ Parse m4sc data\n",
      "++ Finished parsing m4sc data\n",
      "(10000, 7000)\n",
      "(10000,)\n",
      "[125 205  27 ...  13 243   5]\n",
      "++ Normalizing traces of m4sc\n",
      "++ Organizing traces\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4671945746b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# loads traces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"+ Commense loading data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_database_pickle_normalized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_database\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;31m# X, y = load_database_DPA(my_database, meta_db)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"++ Data dimestions are: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-da08fefec389>\u001b[0m in \u001b[0;36mload_database_pickle_normalized\u001b[0;34m(my_database)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtrace_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# number of traces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__set_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "#ASCAD: Adapted by Mahmoud Gharra\n",
    "\n",
    "# NOTE: code could take another database if it has the same format as the one provided.\n",
    "# NOTE: you need to make a local directory with the name of your chosen training_model\n",
    "\n",
    "###################\n",
    "###################\n",
    "# Hyper parameters:\n",
    "###################\n",
    "\n",
    "tot = 3 # total number of random samples that you'd like to display\n",
    "f_s = 100 # frequency of data in Hz --- I can't seem to find this value in proj\n",
    "\n",
    "tst_len = 30 # length of testing set\n",
    "\n",
    "###################\n",
    "###################\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "if len(sys.argv)!=2:\n",
    "    #default parameters values\n",
    "#     my_database = \"PRE_MAY_06/projects/operand_scanning_32\"\n",
    "#     DB_title = \"operand_scanning_32\"\n",
    "\n",
    "#     my_database = \"schoolbook32/schoolbook32\"        \n",
    "#     my_database = \"../2020_APR_23/polymul32/projects/operand_scanning_32\"\n",
    "#     DB_title = \"schoolbook32\"\n",
    "\n",
    "#     my_database = \"../GSprojectNils/projects/GStraces\"\n",
    "#     my_database = \"../2020_APR_23/polymul32/projects/operand_scanning_32\"\n",
    "#     DB_title = \"Gaussian Sampler\"\n",
    "\n",
    "#     my_database = \"../DPA/DPA_contestv4_rsm/00000\"\n",
    "#     meta_db = \"../DPA/dpav4_rsm_index.txt\"\n",
    "#     DB_title = \"DPA\"\n",
    "\n",
    "#     my_database = \"../GSprojectNils/projects/GStraces\"\n",
    "#     my_database = \"../2020_APR_23/polymul32/projects/operand_scanning_32\"\n",
    "    my_database = \"../../M4SC_1_2020_07_16_155036_8f582681_A.pickle\"\n",
    "#     my_database = \"../../GS_traces_one_rej.zip\"\n",
    "    \n",
    "    DB_title = \"Gaussian Sampler\"\n",
    "    \n",
    "    # TRAINING MODEL IS THE FILE, IN WHICH THE DATA SHOULD BE SAVED\n",
    "else:\n",
    "    #get parameters from user input\n",
    "    DB_title, database = read_parameters_from_file(sys.argv[1])\n",
    "\n",
    "# loads traces\n",
    "print(\"+ Commense loading data\")\n",
    "X, y = load_database_pickle_normalized(my_database)\n",
    "# X, y = load_database_DPA(my_database, meta_db)\n",
    "print(\"++ Data dimestions are: \", np.array(X).shape)\n",
    "\n",
    "print(\"+ sample {0} traces\".format(tot))\n",
    "if tot != 0:\n",
    "    sample_traces(X)\n",
    "    \n",
    "save_db(DB_title, traces=X, labels=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks like we have PowerLine intereference:\n",
    "# This should fix it but I can't download matlab...\n",
    "# I need conda for it, which I also don't have\n",
    "# -- https://github.com/mrezak/removePLI\n",
    "# import matlab.engine\n",
    "# eng = matlab.engine.start_matlab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High and low pass filtering https://mne.tools/0.13/auto_tutorials/plot_artifacts_correction_filtering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project = cw.open_project(my_database)\n",
    "\n",
    "# print('location (project.location): ', project.location)\n",
    "# print()\n",
    "\n",
    "# print('trace length (project.waves[0].size): ', project.waves[0].size)\n",
    "# print()\n",
    "\n",
    "# print('number of traces (len(project.traces)): ', len(project.traces))\n",
    "# print()\n",
    "# print()\n",
    "\n",
    "# # Iterable for working with only the text in.\n",
    "# # Each item in the iterable is a byte array.\n",
    "# # Supports iterating, indexing, and slicing:\n",
    "# print(\"project.textins[0].size\", project.textins[0].size)\n",
    "# print(\"project.textins[0]\", project.textins[0])\n",
    "# print(\"project.textins (an iterable of length 64): \", project.textins)\n",
    "# print()\n",
    "# print(\"len(project.textins[0].tobytes())\", len(project.textins[0].tobytes()))\n",
    "# print()\n",
    "# print()\n",
    "\n",
    "# # Iterable for working with only the text out.\n",
    "# # Each item in the iterable is a byte array.\n",
    "# # Supports iterating, indexing, and slicing:\n",
    "# print(\"project.textouts[0] (always False ... What is it?): \", project.textouts[0])\n",
    "# print()\n",
    "# print()\n",
    "\n",
    "# # An output for all 64 keys with . \n",
    "# print(\"num. of keys (project.keys[0].size): \", project.keys[0].size)\n",
    "# print(\"max index of labelled traces (project.keys.max): \", project.keys.max)\n",
    "# print(\"number of classes\")\n",
    "# print()\n",
    "\n",
    "# # keys_set = set(project.keys[0])\n",
    "\n",
    "\n",
    "# # the following 4 lines are for the Gaussian sampler\n",
    "# keys_set = set([])\n",
    "# for i in range(len(project.keys[0])):\n",
    "#     keys_set = keys_set.union(set(project.keys[i]))\n",
    "\n",
    "\n",
    "# print(\"keys_set = set(project.keys[0]) -- (not well implemented... Assumes all key labels are in first trace):\", keys_set)\n",
    "# print(\"number of classes (based on keys_set) -- (len(keyset)): \", len(keys_set))\n",
    "# print()\n",
    "# print()\n",
    "\n",
    "# print(\"file name (project.get_filename()): \", project.get_filename())\n",
    "# print()\n",
    "\n",
    "# print(\"length of singular traces (project.trace_manager().num_points()): \", project.trace_manager().num_points())\n",
    "# print(\"number total of traces (project.trace_manager().num_traces()): \", project.trace_manager().num_traces())\n",
    "# print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://stackoverflow.com/questions/50590175/how-to-create-multiple-plots-with-scrollable-x-axis-with-a-single-slider\n",
    "\n",
    "# %matplotlib inline\n",
    "# from ipywidgets import interactive\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# def f(m):\n",
    "#     plt.figure(2)\n",
    "#     x = np.linspace(0, 1000, num=2000)\n",
    "#     plt.plot(x,x)\n",
    "#     #plt.plot(x, m * x)\n",
    "#     plt.xlim(m, m+100)\n",
    "#     plt.show()\n",
    "\n",
    "# interactive(f, m=(0, 900.0))\n",
    "\n",
    "\n",
    "# def f2(m):\n",
    "#     t = np.arange(X.shape[1])\n",
    "#     myTraces = np.array(X)\n",
    "\n",
    "# #     plt.plot(x,x)\n",
    "#     plt.plot(t, myTraces[243])\n",
    "#     plt.xlim(m, m+500)\n",
    "#     plt.show()\n",
    "\n",
    "# interactive(f2, m=(0, 4500.0))\n",
    "\n",
    "\n",
    "\n",
    "# def f3(m):\n",
    "#     t = np.arange(X.shape[1])\n",
    "#     myTraces = np.array(X)\n",
    "\n",
    "# #     plt.plot(x,x)\n",
    "#     plt.plot(t, myTraces[563])\n",
    "#     plt.xlim(m, m+500)\n",
    "#     plt.show()\n",
    "\n",
    "# interactive(f3, m=(0, 4500.0))\n",
    "\n",
    "\n",
    "\n",
    "# def f4(m):\n",
    "#     t = np.arange(X.shape[1])\n",
    "#     myTraces = np.array(X)\n",
    "\n",
    "# #     plt.plot(x,x)\n",
    "#     plt.plot(t, myTraces[1000])\n",
    "#     plt.xlim(m, m+500)\n",
    "#     plt.show()\n",
    "\n",
    "# interactive(f4, m=(0, 4500.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # returns traces and labels, as well as some important global meta_data\n",
    "\n",
    "# def load_database2(my_database):\n",
    "#     # load traces\n",
    "#     print(\"++ Loading projects\")\n",
    "#     project = cw.open_project(my_database)\n",
    "#     global KEY_LENGTH, TEXT_LENGTH, SAMPLE_HIGH, TRAINING_SLICE, TEST_NUM, TEST_SLICE\n",
    "\n",
    "#     # Organize trace data for MLP\n",
    "#     print(\"++ Organizing traces\")\n",
    "#     KEY_LENGTH = TEXT_LENGTH = project.keys[0].size\n",
    "\n",
    "#     sample_low = 0\n",
    "#     SAMPLE_HIGH = project.trace_manager().num_points() # length of singular trace\n",
    "#     sample_slice = slice(sample_low, SAMPLE_HIGH)\n",
    "#     sample_num = len(project.traces) # number of traces\n",
    "# #     print(\"sample num: \", sample_num)\n",
    "#     training_num = sample_num - tst_len\n",
    "#     TRAINING_SLICE = slice(0, training_num)\n",
    "\n",
    "#     TEST_NUM = sample_num - training_num\n",
    "#     TEST_SLICE = slice(training_num, TEST_NUM + training_num)\n",
    "#     assert TEST_NUM + training_num <= sample_num\n",
    "#     assert training_num > 3*TEST_NUM\n",
    "\n",
    "#     # organize traces in X matrix\n",
    "#     X = empty(shape=(sample_num, SAMPLE_HIGH - sample_low))\n",
    "#     for i in range(sample_num):\n",
    "#         X[i] = project.waves[i][sample_slice]\n",
    "\n",
    "#     # organize the operands in Y matrix\n",
    "#     y = empty(shape=(sample_num, KEY_LENGTH))\n",
    "#     for i in range(sample_num):\n",
    "#         # reproduce values\n",
    "#         y[i] = project.keys[i]\n",
    "# #         text_num = text_gen(seed=i)[:TEXT_LENGTH]\n",
    "# #         key_num = key_gen(seed=i)[:KEY_LENGTH]\n",
    "# #         y[i][:] = key_num[:]\n",
    "#         #y[i][y[i] > 2**15] -= 2**16\n",
    "\n",
    "    \n",
    "#     # transform generated key numbers to labels\n",
    "#     unique = np.unique(y)\n",
    "#     class_dic = dict([[unique[i], i] for i in range(len(unique))])\n",
    "#     y_labelled = np.vectorize(class_dic.get)(y)\n",
    "\n",
    "#     y_orig = y\n",
    "#     return X, y_labelled, y_orig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_new, y_l, y_or = load_database2(my_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_or.shape)\n",
    "# print(y_or[0].shape)\n",
    "# print(y_or[0])\n",
    "\n",
    "# print()\n",
    "\n",
    "# print(y_or[1].shape)\n",
    "# print(y_or[1])\n",
    "\n",
    "# print()\n",
    "\n",
    "# print(np.array((y_or[0]*(2**8)) + y_or[1], dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_or[0])\n",
    "# # print(y_or[0][0::2])\n",
    "# # print(y_or[0][::2][8]) # even indices\n",
    "# # print(y_or[0][1::2][8]) # odd indices\n",
    "# print(np.array((y_or.T[1::2].T*(2**8)) + y_or.T[::2].T, dtype=np.int16).shape)\n",
    "# print(np.array((y_or.T[1::2].T*(2**8)) + y_or.T[::2].T, dtype=np.int16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# print(np.sum(np.array((y_or.T[1::2].T*(2**8)) + y_or.T[::2].T, dtype=np.int16) == -1))\n",
    "# np.array((y_or.T[1::2].T*(2**8)) + y_or.T[::2].T, dtype=np.int16).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15000 * 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90180 / 480000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_database = \"../GSproject/projects/GStraces\"\n",
    "# project = cw.open_project(my_database)\n",
    "\n",
    "# print(\"total number of traces: {}\".format(project.keys.max + 1))\n",
    "\n",
    "# # see if all keys have correct length (32)\n",
    "# count = 0\n",
    "# myDict = dict()\n",
    "# for i in range(project.keys.max):\n",
    "#     if (len(np.asarray(bytearray(project.keys[i])))) is not 32:\n",
    "#         myDict[i]= len(np.asarray(bytearray(project.keys[i])))\n",
    "#         count += 1\n",
    "\n",
    "# print(\"number of traces not containing 32 instances of our 8-bit key: {}\".format(count)) # should be 0\n",
    "# print(\"indices of problematic traces and their length (ind: length): {}\".format(myDict)) # should be empty\n",
    "\n",
    "\n",
    "# # quick sanity check for all key instances\n",
    "# mySet = set([])\n",
    "# for i in range(project.keys.max):\n",
    "#     mySet = mySet.union(np.unique(np.asarray(bytearray(project.keys[i]))))\n",
    "\n",
    "# print(\"appearing key instances (256 is correct): {}\".format(len(mySet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_database = \"../GSproject/projects/GStraces\"\n",
    "# project = cw.open_project(my_database)\n",
    "\n",
    "# print(\"total number of traces: {}\".format(project.keys.max + 1))\n",
    "\n",
    "# # see if all keys have correct length (32)\n",
    "# count = 0\n",
    "# myDict = dict()\n",
    "# KEY_LENGTH = 32 * 8\n",
    "# sample_num = len(project.traces)\n",
    "\n",
    "# y = empty(shape=(sample_num, KEY_LENGTH * 8))\n",
    "\n",
    "# # np.asarray(bytearray(project.keys[0]))\n",
    "\n",
    "# # ''.join(['{0:08b}'.format(ff) for ff in np.asarray(bytearray(project.keys[0]))])\n",
    "\n",
    "# for i in range(50):\n",
    "#     print(np.asarray([int(char) for char in ''.join(['{0:08b}'.format(ff) for ff in np.asarray(bytearray(project.keys[i]))])])[0:30])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import codecs\n",
    "# encodings = ['utf-8', 'windows-1250', 'windows-1252', 'utf-16', \"ascii\", 'cp037']# ...etc]\n",
    "# for e in encodings:\n",
    "#     try:\n",
    "#         fh = project.keys[0].decode(e)\n",
    "#     except UnicodeDecodeError:\n",
    "#         print('got unicode error with %s , trying different encoding' % e)\n",
    "#     else:\n",
    "#         print('opening the file with encoding:  %s ' % e)\n",
    "#         print('it looks like this: {}'.format(fh))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bytearray(b'0q(K\\xa6\\xd5\\xb7C\\xefx\\x8fq\\x87\\x9c\\xd0)\\x86a\\x05V\\x14\\x03#\\xa9!b\\xf9\\r\\xd0\\xd0v\\xcf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.asarray(bytearray(b'0q(K\\xa6\\xd5\\xb7C\\xefx\\x8fq\\x87\\x9c\\xd0)\\x86a\\x05V\\x14\\x03#\\xa9!b\\xf9\\r\\xd0\\xd0v\\xcf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.asarray(bytearray(b'0q(K\\xa6\\xd5\\xb7C\\xefx\\x8fq\\x87\\x9c\\xd0)\\x86a\\x05V\\x14\\x03#\\xa9!b\\xf9\\r\\xd0\\xd0v\\xcf')).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# a = np.array([[1, 2, 3, 4],\n",
    "# [5, 6, 7, 8],\n",
    "# [9, 10, 11, 12],\n",
    "# [13, 14, 15, 16]])\n",
    "\n",
    "# count = 0\n",
    "# print(\"a:\\n{}\".format(a))\n",
    "# for i in range(1, 5):\n",
    "#     print(\"a[0:-{}]:\\n{}\".format(i, a[0:-i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # advs = np.load(\"./training_GAUSS_bitwise_mlp_batch100_epochs2_MAXATT1_lr1e-4_valid1e-1/tst_acc.npy\", allow_pickle=True)\n",
    "# advs = np.load(\"./training_GAUSS_bitwise_cnn_batch100_epochs5_MAXATT1_lr1e-4_valid1e-1/tst_acc.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(advs.tolist().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# advs.tolist().values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "data = []\n",
    "data_file = \"../../M4SC_1_2020_07_16_155036_8f582681_A.pickle\"\n",
    "with open(data_file, 'rb') as f:\n",
    "    while True:\n",
    "        try:\n",
    "            data.append(pickle.load(f))\n",
    "        except EOFError:\n",
    "            break\n",
    "\n",
    "# parse traces - convert int array to bit array\n",
    "print(\"++ Sample Pickle data\")\n",
    "print(\"len(data): {}\".format(len(data)))\n",
    "\n",
    "el = 0\n",
    "for i in range(10):\n",
    "    print(\"data[{}][{}]: {}\".format(i, el, data[i][el]))\n",
    "\n",
    "    \n",
    "for i in range(10):\n",
    "    print(\"data[{}][{}][:2]: {}\".format(i, el, data[i][el][:2]))\n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"len(data[{}][{}]): {}\".format(0, el, len(data[0][el])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"++ Parse m4sc data\")\n",
    "X = np.array([i[4] for i in data])\n",
    "y_init = np.array([int((int(i[0][:2], 16))) for i in data])\n",
    "print(\"++ Finished parsing m4sc data\")\n",
    "\n",
    "print(X.shape)\n",
    "print(y_init.shape)\n",
    "\n",
    "\n",
    "print(\"min(np.unique(y_init)): \", min(np.unique(y_init)))\n",
    "print(\"max(np.unique(y_init)): \", max(np.unique(y_init)))\n",
    "\n",
    "y = np.array([bit8(i) for i in y_init])\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these functions were used in the loading of m4sc data\n",
    "def bit8(i):\n",
    "    return [(0 if (0 == (i & (1 << j))) else 1) for j in range(8)]\n",
    "\n",
    "def concatBits(a, b):\n",
    "    return 2*a + b\n",
    "\n",
    "def chunk4(i):\n",
    "    temp = bit8(i)\n",
    "    return [concatBits(temp[2*i], temp[2*i + 1]) for i in range(4)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bit8: {}\".format(bit8(y_init[0])))\n",
    "\n",
    "print(\"chunk4: {}\".format(chunk4(y_init[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
